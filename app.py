# app.py
import streamlit as st
from pathlib import Path
import traceback

st.set_page_config(page_title="ë‹¬ & íƒœì–‘ê³„ í†µí•© ì±—ë´‡", page_icon="ğŸŒ™", layout="wide")

# 0) í™”ë©´ ìƒë‹¨ íˆì–´ë¡œ ì˜ì—­
st.markdown(
    """
    <style>
      .hero {padding: 24px 8px 0 8px;}
      .hero h1 {font-size: 40px; margin: 0 0 8px 0;}
      .hero p {color: #cbd5e1; margin: 0 0 16px 0;}
    </style>
    <div class="hero">
      <h1>ğŸŒ™ ë‹¬ & ğŸª íƒœì–‘ê³„ í†µí•© ì±—ë´‡</h1>
      <p>ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!</p>
    </div>
    """,
    unsafe_allow_html=True,
)

# 1) prompt.yaml ì½ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©, ì—ëŸ¬ ì‹œì—ë„ ê³„ì† ì§„í–‰)
PROMPTS = {
    "moon": {"system": "ë„ˆëŠ” 'ë‹¬ë°•ì‚¬ ë£¨ë‚˜'ì•¼. ì´ˆë“± 4í•™ë…„ ëˆˆë†’ì´, 3~4ë¬¸ì¥, ì‰¬ìš´ ë§ë¡œ ë‹µí•´."},
    "solar": {"system": "ë„ˆëŠ” 'ìš°ì£¼ë„¤ë¹„'ì•¼. íƒœì–‘ê³„/í–‰ì„±/ìœ„ì„± ì£¼ì œ, 3~4ë¬¸ì¥, ì‰¬ìš´ ë§ë¡œ ë‹µí•´."},
    "guardrails": {
        "refuse_unrelated": "ë‹¬/íƒœì–‘ê³„ì™€ ì§ì ‘ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì´ë©´ ì£¼ì œì™€ ì—°ê²°ë˜ê²Œ ë¶€ë“œëŸ½ê²Œ ìœ ë„í•´."
    },
}

try:
    import yaml
    p = Path(__file__).with_name("prompt.yaml")
    if p.exists():
        with p.open(encoding="utf-8") as f:
            loaded = yaml.safe_load(f) or {}
            for k in PROMPTS.keys():
                if k in loaded:
                    PROMPTS[k].update(loaded[k] or {})
except Exception as e:
    st.warning("`prompt.yaml`ì„ ì½ëŠ” ì¤‘ ë¬¸ì œê°€ ìˆì—ˆì§€ë§Œ, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
    with st.expander("í”„ë¡¬í”„íŠ¸ ë¡œë”© ê²½ê³  ìì„¸íˆ ë³´ê¸°"):
        st.code("".join(traceback.format_exc()))

# 2) OpenAI í´ë¼ì´ì–¸íŠ¸ (ì—†ì–´ë„ UIëŠ” ëœ¸)
client = None
model_name = "gpt-4o-mini"
try:
    from openai import OpenAI
    # Streamlit Cloudì—ì„œëŠ” secrets, ë¡œì»¬ì€ secrets.toml ì‚¬ìš©
    api_key = st.secrets.get("OPENAI_API_KEY", None)
    if api_key:
        client = OpenAI(api_key=api_key)
    else:
        st.info("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹µë³€ì€ ì˜ˆì‹œ/ì—ì½”ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
except Exception:
    st.info("âš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (íŒ¨í‚¤ì§€/í‚¤ ë¯¸ì„¤ì •)")

# 3) ì„¸ì…˜ ìƒíƒœ ì•ˆì „ ì´ˆê¸°í™”
for key, default in {
    "page": "ë‹¬ ì±—ë´‡",
    "moon_history": [],
    "solar_history": [],
}.items():
    if key not in st.session_state:
        st.session_state[key] = default

# 4) ì‚¬ì´ë“œë°” ë¼ìš°íŒ… (í•­ìƒ í‘œì‹œ)
with st.sidebar:
    page = st.radio("ğŸ”­ ëª¨ë“œ ì„ íƒ", ["ë‹¬ ì±—ë´‡", "íƒœì–‘ê³„ ì±—ë´‡"], index=0 if st.session_state["page"]=="ë‹¬ ì±—ë´‡" else 1)
    if page != st.session_state["page"]:
        st.session_state["page"] = page
        st.rerun()

# 5) ê³µí†µ ë³´ì¡° í•¨ìˆ˜
def call_llm(messages, sys_prompt):
    """API í‚¤ ì—†ìœ¼ë©´ ì˜ˆì‹œë¡œ ë™ì‘, ìˆìœ¼ë©´ ì‹¤ì œ í˜¸ì¶œ"""
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ prepend
    msgs = [{"role":"system", "content": sys_prompt}] + messages
    if client is None:
        # ë°ëª¨ìš© ì‘ë‹µ
        user_last = next((m["content"] for m in reversed(messages) if m["role"]=="user"), "ì§ˆë¬¸")
        return f"(ë°ëª¨ ì‘ë‹µ) '{user_last}'ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ ì¤„ê²Œ! ì•„ì§ API í‚¤ê°€ ì—†ì–´ ì˜ˆì‹œë¡œ ë‹µí–ˆì–´. ğŸ˜‰"
    try:
        resp = client.chat.completions.create(
            model=model_name,
            messages=msgs,
            temperature=0.3,
            max_tokens=400,
        )
        return resp.choices[0].message.content
    except Exception:
        st.error("ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ì•„ë˜ ë¡œê·¸ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.code("".join(traceback.format_exc()))
        return None

def render_chat(history_key, sys_prompt, quick_questions):
    """ê³µí†µ ì±„íŒ… UI: íˆìŠ¤í† ë¦¬ í‘œì‹œ + ë¹ ë¥¸ ì§ˆë¬¸ + ì…ë ¥"""
    # ë¹ ë¥¸ ì§ˆë¬¸ ë²„íŠ¼
    st.markdown("#### ğŸš€ ë¹ ë¥¸ ì§ˆë¬¸")
    cols = st.columns(4)
    for i, (label, question) in enumerate(quick_questions.items()):
        if cols[i % 4].button(label, use_container_width=True):
            st.session_state[history_key].append({"role":"user", "content":question})

    st.divider()

    # íˆìŠ¤í† ë¦¬ ì¶œë ¥
    for msg in st.session_state[history_key]:
        avatar = "ğŸŒ™" if msg["role"]=="assistant" else "ğŸ‘©â€ğŸ“"
        with st.chat_message("assistant" if msg["role"]=="assistant" else "user", avatar=avatar):
            st.write(msg["content"])

    # ì…ë ¥ì°½
    user_text = st.chat_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•œê°€ìš”?")
    if user_text:
        st.session_state[history_key].append({"role":"user", "content":user_text})

    # ìƒˆ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ë‹µë³€ ìƒì„±
    if st.session_state[history_key] and st.session_state[history_key][-1]["role"]=="user":
        # ê°€ë“œë ˆì¼: ì£¼ì œ ìœ ë„
        guard = PROMPTS["guardrails"]["refuse_unrelated"]
        # LLM í˜¸ì¶œ
        answer = call_llm(st.session_state[history_key][-6:], sys_prompt + "\n\n" + guard)
        if answer:
            st.session_state[history_key].append({"role":"assistant", "content":answer})
            with st.chat_message("assistant", avatar="ğŸŒ™"):
                st.write(answer)

# 6) í˜ì´ì§€ ë³„ ë Œë”
if st.session_state["page"] == "ë‹¬ ì±—ë´‡":
    st.subheader("ğŸŒ™ ë‹¬ ì±—ë´‡ (ë‹¬ë°•ì‚¬ ë£¨ë‚˜)")
    moon_quick = {
        "ğŸŒ— ë‹¬ì˜ ìœ„ìƒ": "ë‹¬ì˜ ëª¨ì–‘ì´ ì™œ ë§¤ì¼ ë‹¬ë¼ì ¸?",
        "ğŸ”­ ê´€ì°° íŒ": "ì´ˆë“±í•™ìƒì´ ë‹¬ì„ ê´€ì°°í•  ë•Œ ë­ê°€ ì¤‘ìš”í•´?",
        "ğŸ“– ì „ì„¤ í•˜ë‚˜": "ë‹¬ê³¼ ê´€ë ¨ëœ ì „ì„¤ í•œ ê°€ì§€ë§Œ ë“¤ë ¤ì¤˜.",
        "ğŸ›¡ï¸ ì•ˆì „ ìˆ˜ì¹™": "ë°¤ì— ë‹¬ ê´€ì°°í•  ë•Œ ì£¼ì˜í•  ì  3ê°€ì§€ë¥¼ ì•Œë ¤ì¤˜.",
    }
    render_chat("moon_history", PROMPTS["moon"]["system"], moon_quick)
else:
    st.subheader("ğŸª íƒœì–‘ê³„ ì±—ë´‡ (ìš°ì£¼ë„¤ë¹„)")
    solar_quick = {
        "â˜€ï¸ íƒœì–‘": "íƒœì–‘ì€ ì–´ë–¤ ë³„ì´ì•¼? í¬ê¸°ì™€ ì—­í• ì€?",
        "ğŸª í–‰ì„± ìˆœì„œ": "ìˆ˜~í•´ í–‰ì„± ìˆœì„œë¥¼ ì™¸ìš°ëŠ” ì‰¬ìš´ ë°©ë²• ì•Œë ¤ì¤˜.",
        "ğŸŒ ì§€êµ¬ íŠ¹ì§•": "ì§€êµ¬ê°€ ë‹¤ë¥¸ í–‰ì„±ê³¼ ë‹¤ë¥¸ ì  3ê°€ì§€!",
        "ğŸ›°ï¸ ìœ„ì„±": "ìœ„ì„±ì€ ë­ì•¼? ìì—°ìœ„ì„±ê³¼ ì¸ê³µìœ„ì„± ì°¨ì´ë„!",
    }
    render_chat("solar_history", PROMPTS["solar"]["system"], solar_quick)
